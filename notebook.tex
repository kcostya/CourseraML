
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{PA\_linreg\_stochastic\_grad\_descent}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \section{Линейная регрессия и стохастический градиентный
спуск}\label{ux43bux438ux43dux435ux439ux43dux430ux44f-ux440ux435ux433ux440ux435ux441ux441ux438ux44f-ux438-ux441ux442ux43eux445ux430ux441ux442ux438ux447ux435ux441ux43aux438ux439-ux433ux440ux430ux434ux438ux435ux43dux442ux43dux44bux439-ux441ux43fux443ux441ux43a}

    Задание основано на материалах лекций по линейной регрессии и
градиентному спуску. Вы будете прогнозировать выручку компании в
зависимости от уровня ее инвестиций в рекламу по TV, в газетах и по
радио.

    \subsection{Вы
научитесь:}\label{ux432ux44b-ux43dux430ux443ux447ux438ux442ux435ux441ux44c}

\begin{itemize}
\tightlist
\item
  решать задачу восстановления линейной регрессии
\item
  реализовывать стохастический градиентный спуск для ее настройки
\item
  решать задачу линейной регрессии аналитически
\end{itemize}

    \subsection{Введение}\label{ux432ux432ux435ux434ux435ux43dux438ux435}

Линейная регрессия - один из наиболее хорошо изученных методов машинного
обучения, позволяющий прогнозировать значения количественного признака в
виде линейной комбинации прочих признаков с параметрами - весами модели.
Оптимальные (в смысле минимальности некоторого функционала ошибки)
параметры линейной регрессии можно найти аналитически с помощью
нормального уравнения или численно с помощью методов оптимизации.

    Линейная регрессия использует простой функционал качества -
среднеквадратичную ошибку. Мы будем работать с выборкой, содержащей 3
признака. Для настройки параметров (весов) модели решается следующая
задача:
\[\Large \frac{1}{\ell}\sum_{i=1}^\ell{{((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}^2} \rightarrow \min_{w_0, w_1, w_2, w_3},\]
где \(x_{i1}, x_{i2}, x_{i3}\) - значения признаков \(i\)-го объекта,
\(y_i\) - значение целевого признака \(i\)-го объекта, \(\ell\) - число
объектов в обучающей выборке.

    \subsection{Градиентный
спуск}\label{ux433ux440ux430ux434ux438ux435ux43dux442ux43dux44bux439-ux441ux43fux443ux441ux43a}

Параметры \(w_0, w_1, w_2, w_3\), по которым минимизируется
среднеквадратичная ошибка, можно находить численно с помощью
градиентного спуска. Градиентный шаг для весов будет выглядеть следующим
образом:
\[\Large w_0 \leftarrow w_0 - \frac{2\eta}{\ell} \sum_{i=1}^\ell{{((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}}\]
\[\Large w_j \leftarrow w_j - \frac{2\eta}{\ell} \sum_{i=1}^\ell{{x_{ij}((w_0 + w_1x_{i1} + w_2x_{i2} +  w_3x_{i3}) - y_i)}},\ j \in \{1,2,3\}\]
Здесь \(\eta\) - параметр, шаг градиентного спуска.

    \subsection{Стохастический градиентный
спуск}\label{ux441ux442ux43eux445ux430ux441ux442ux438ux447ux435ux441ux43aux438ux439-ux433ux440ux430ux434ux438ux435ux43dux442ux43dux44bux439-ux441ux43fux443ux441ux43a}

Проблема градиентного спуска, описанного выше, в том, что на больших
выборках считать на каждом шаге градиент по всем имеющимся данным может
быть очень вычислительно сложно. В стохастическом варианте градиентного
спуска поправки для весов вычисляются только с учетом одного случайно
взятого объекта обучающей выборки:
\[\Large w_0 \leftarrow w_0 - \frac{2\eta}{\ell} {((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)}\]
\[\Large w_j \leftarrow w_j - \frac{2\eta}{\ell} {x_{kj}((w_0 + w_1x_{k1} + w_2x_{k2} +  w_3x_{k3}) - y_k)},\ j \in \{1,2,3\},\]
где \(k\) - случайный индекс, \(k \in \{1, \ldots, \ell\}\).

    \subsection{Нормальное
уравнение}\label{ux43dux43eux440ux43cux430ux43bux44cux43dux43eux435-ux443ux440ux430ux432ux43dux435ux43dux438ux435}

Нахождение вектора оптимальных весов \(w\) может быть сделано и
аналитически. Мы хотим найти такой вектор весов \(w\), чтобы вектор
\(y\), приближающий целевой признак, получался умножением матрицы \(X\)
(состоящей из всех признаков объектов обучающей выборки, кроме целевого)
на вектор весов \(w\). То есть, чтобы выполнялось матричное уравнение:
\[\Large y = Xw\] Домножением слева на \(X^T\) получаем:
\[\Large X^Ty = X^TXw\] Это хорошо, поскольку теперь матрица \(X^TX\) -
квадратная, и можно найти решение (вектор \(w\)) в виде:
\[\Large w = {(X^TX)}^{-1}X^Ty\] Матрица \({(X^TX)}^{-1}X^T\) -
\href{https://ru.wikipedia.org/wiki/Псевдообратная_матрица}{\emph{псевдообратная}}
для матрицы \(X\). В NumPy такую матрицу можно вычислить с помощью
функции
\href{http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.linalg.pinv.html}{numpy.linalg.pinv}.

Однако, нахождение псевдообратной матрицы - операция вычислительно
сложная и нестабильная в случае малого определителя матрицы \(X\)
(проблема мультиколлинеарности). На практике лучше находить вектор весов
\(w\) решением матричного уравнения \[\Large X^TXw = X^Ty\]Это может
быть сделано с помощью функции
\href{http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.linalg.solve.html}{numpy.linalg.solve}.

Но все же на практике для больших матриц \(X\) быстрее работает
градиентный спуск, особенно его стохастическая версия.

    \subsection{Инструкции по
выполнению}\label{ux438ux43dux441ux442ux440ux443ux43aux446ux438ux438-ux43fux43e-ux432ux44bux43fux43eux43bux43dux435ux43dux438ux44e}

    В начале напишем простую функцию для записи ответов в текстовый файл.
Ответами будут числа, полученные в ходе решения этого задания,
округленные до 3 знаков после запятой. Полученные файлы после выполнения
задания надо отправить в форму на странице задания на Coursera.org.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k}{def} \PY{n+nf}{write\PYZus{}answer\PYZus{}to\PYZus{}file}\PY{p}{(}\PY{n}{answer}\PY{p}{,} \PY{n}{filename}\PY{p}{)}\PY{p}{:}
            \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{f\PYZus{}out}\PY{p}{:}
                \PY{n}{f\PYZus{}out}\PY{o}{.}\PY{n}{write}\PY{p}{(}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{answer}\PY{p}{,} \PY{l+m+mi}{3}\PY{p}{)}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \textbf{1. Загрузите данные из файла \emph{advertising.csv} в объект
pandas DataFrame.
\href{http://www-bcf.usc.edu/~gareth/ISL/data.html}{Источник данных}.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k+kn}{import} \PY{n+nn}{pandas} \PY{k+kn}{as} \PY{n+nn}{pd}
        \PY{n}{adver\PYZus{}data} \PY{o}{=} \PY{n}{pd}\PY{o}{.}\PY{n}{read\PYZus{}csv}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{advertising.csv}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \textbf{Посмотрите на первые 5 записей и на статистику признаков в этом
наборе данных.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{n}{adver\PYZus{}data}\PY{o}{.}\PY{n}{head}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}3}]:}       TV  Radio  Newspaper  Sales
        1  230.1   37.8       69.2   22.1
        2   44.5   39.3       45.1   10.4
        3   17.2   45.9       69.3    9.3
        4  151.5   41.3       58.5   18.5
        5  180.8   10.8       58.4   12.9
\end{Verbatim}
            
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{o}{\PYZpc{}}\PY{k}{matplotlib} inline
        \PY{n}{adver\PYZus{}data}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{TV distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}4}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fe20587b2d0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_15_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{n}{adver\PYZus{}data}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Radio}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Radio distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}5}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fe205896290>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_16_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{n}{adver\PYZus{}data}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{y}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Newspaper}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{kind}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{hist}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} 
                   \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}  \PY{n}{title}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Newspaper distribution}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}6}]:} <matplotlib.axes.\_subplots.AxesSubplot at 0x7fe2037657d0>
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Создайте массивы NumPy \emph{X} из столбцов TV, Radio и
Newspaper и \emph{y} - из столбца Sales. Используйте атрибут
\emph{values} объекта pandas DataFrame.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{n}{X} \PY{o}{=} \PY{n}{adver\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{TV}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Radio}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Newspaper}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{values}
        \PY{n}{y} \PY{o}{=} \PY{n}{adver\PYZus{}data}\PY{p}{[}\PY{p}{[}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Sales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{]}\PY{p}{]}\PY{o}{.}\PY{n}{values}
\end{Verbatim}


    \textbf{Отмасштабируйте столбцы матрицы \emph{X}, вычтя из каждого
значения среднее по соответствующему столбцу и поделив результат на
стандартное отклонение. Для определенности, используйте методы mean и
std векторов NumPy (реализация std в Pandas может отличаться). Обратите
внимание, что в numpy вызов функции .mean() без параметров возвращает
среднее по всем элементам массива, а не по столбцам, как в pandas. Чтобы
произвести вычисление по столбцам, необходимо указать параметр axis.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{n}{means}\PY{p}{,} \PY{n}{stds} \PY{o}{=} \PY{n}{X}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{o}{.}\PY{n}{std}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{n}{X} \PY{o}{=} \PY{p}{(}\PY{n}{X}\PY{o}{\PYZhy{}}\PY{n}{means}\PY{p}{)}\PY{o}{/}\PY{n}{stds}
\end{Verbatim}


    \textbf{Добавьте к матрице \emph{X} столбец из единиц, используя методы
\emph{hstack}, \emph{ones} и \emph{reshape} библиотеки NumPy. Вектор из
единиц нужен для того, чтобы не обрабатывать отдельно коэффициент
\(w_0\) линейной регрессии.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k+kn}{as} \PY{n+nn}{np}
         \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{hstack}\PY{p}{(}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{)}\PY{p}{)}
\end{Verbatim}


    \textbf{2. Реализуйте функцию \emph{mserror} - среднеквадратичную ошибку
прогноза. Она принимает два аргумента - объекты Series \emph{y}
(значения целевого признака) и \emph{y\_pred} (предсказанные значения).
Не используйте в этой функции циклы - тогда она будет вычислительно
неэффективной.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{k}{def} \PY{n+nf}{mserror}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{p}{(}\PY{p}{(}\PY{n}{y} \PY{o}{\PYZhy{}} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \textbf{Какова среднеквадратичная ошибка прогноза значений Sales, если
всегда предсказывать медианное значение Sales по исходной выборке?
Запишите ответ в файл '1.txt'.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{n}{answer1} \PY{o}{=} \PY{n}{mserror}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{np}\PY{o}{.}\PY{n}{median}\PY{p}{(}\PY{n}{y}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{n}{answer1}\PY{p}{)}
         \PY{n}{write\PYZus{}answer\PYZus{}to\PYZus{}file}\PY{p}{(}\PY{n}{answer1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{1.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
28.34575

    \end{Verbatim}

    \textbf{3. Реализуйте функцию \emph{normal\_equation}, которая по
заданным матрицам (массивам NumPy) \emph{X} и \emph{y} вычисляет вектор
весов \(w\) согласно нормальному уравнению линейной регрессии.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{k}{def} \PY{n+nf}{normal\PYZus{}equation}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{pinv}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,}\PY{n}{y}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{n}{norm\PYZus{}eq\PYZus{}weights} \PY{o}{=} \PY{n}{normal\PYZus{}equation}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{n}{norm\PYZus{}eq\PYZus{}weights}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[14.0225    ]
 [ 3.91925365]
 [ 2.79206274]
 [-0.02253861]]

    \end{Verbatim}

    \textbf{Какие продажи предсказываются линейной моделью с весами,
найденными с помощью нормального уравнения, в случае средних инвестиций
в рекламу по ТВ, радио и в газетах? (то есть при нулевых значениях
масштабированных признаков TV, Radio и Newspaper). Запишите ответ в файл
'2.txt'.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{n}{answer2} \PY{o}{=} \PY{n}{norm\PYZus{}eq\PYZus{}weights}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
         \PY{k}{print}\PY{p}{(}\PY{n}{answer2}\PY{p}{)}
         \PY{n}{write\PYZus{}answer\PYZus{}to\PYZus{}file}\PY{p}{(}\PY{n}{answer2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{2.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[14.0225]

    \end{Verbatim}

    \textbf{4. Напишите функцию \emph{linear\_prediction}, которая принимает
на вход матрицу \emph{X} и вектор весов линейной модели \emph{w}, а
возвращает вектор прогнозов в виде линейной комбинации столбцов матрицы
\emph{X} с весами \emph{w}.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{k}{def} \PY{n+nf}{linear\PYZus{}prediction}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{:}
             \PY{k}{return} \PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}
\end{Verbatim}


    \textbf{Какова среднеквадратичная ошибка прогноза значений Sales в виде
линейной модели с весами, найденными с помощью нормального уравнения?
Запишите ответ в файл '3.txt'.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}17}]:} \PY{n}{answer3} \PY{o}{=} \PY{n}{mserror}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{linear\PYZus{}prediction}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{norm\PYZus{}eq\PYZus{}weights}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{n}{answer3}\PY{p}{)}
         \PY{n}{write\PYZus{}answer\PYZus{}to\PYZus{}file}\PY{p}{(}\PY{n}{answer3}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{3.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2.784126314510936

    \end{Verbatim}

    \textbf{5. Напишите функцию \emph{stochastic\_gradient\_step},
реализующую шаг стохастического градиентного спуска для линейной
регрессии. Функция должна принимать матрицу \emph{X}, вектора \emph{y} и
\emph{w}, число \emph{train\_ind} - индекс объекта обучающей выборки
(строки матрицы \emph{X}), по которому считается изменение весов, а
также число \emph{\(\eta\)} (eta) - шаг градиентного спуска (по
умолчанию \emph{eta}=0.01). Результатом будет вектор обновленных весов.
Наша реализация функции будет явно написана для данных с 3 признаками,
но несложно модифицировать для любого числа признаков, можете это
сделать.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}18}]:} \PY{k}{def} \PY{n+nf}{stochastic\PYZus{}gradient\PYZus{}step}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{train\PYZus{}ind}\PY{p}{,} \PY{n}{eta}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{)}\PY{p}{:}
             \PY{n}{x} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{train\PYZus{}ind}\PY{p}{]}
             \PY{n}{dim} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{x}\PY{p}{)}
             \PY{n}{yk} \PY{o}{=} \PY{n}{y}\PY{p}{[}\PY{n}{train\PYZus{}ind}\PY{p}{]}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{L} \PY{o}{=} \PY{n+nb}{float}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{)}
             \PY{k}{return}  \PY{n}{w} \PY{o}{\PYZhy{}} \PY{n}{eta} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{p}{[}\PY{p}{(}\PY{l+m+mi}{2}\PY{o}{/}\PY{n}{L}\PY{p}{)}\PY{o}{*}\PY{n}{x}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{x}\PY{p}{,} \PY{n}{w}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{n}{yk}\PY{p}{)} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{dim}\PY{p}{)}\PY{p}{]}\PY{p}{)}
\end{Verbatim}


    \textbf{6. Напишите функцию \emph{stochastic\_gradient\_descent},
реализующую стохастический градиентный спуск для линейной регрессии.
Функция принимает на вход следующие аргументы:} - X - матрица,
соответствующая обучающей выборке - y - вектор значений целевого
признака - w\_init - вектор начальных весов модели - eta - шаг
градиентного спуска (по умолчанию 0.01) - max\_iter - максимальное число
итераций градиентного спуска (по умолчанию 10000) - max\_weight\_dist -
максимальное евклидово расстояние между векторами весов на соседних
итерациях градиентного спуска, при котором алгоритм прекращает работу
(по умолчанию 1e-8) - seed - число, используемое для воспроизводимости
сгенерированных псевдослучайных чисел (по умолчанию 42) - verbose - флаг
печати информации (например, для отладки, по умолчанию False)

\textbf{На каждой итерации в вектор (список) должно записываться текущее
значение среднеквадратичной ошибки. Функция должна возвращать вектор
весов \(w\), а также вектор (список) ошибок.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}19}]:} \PY{k}{def} \PY{n+nf}{stochastic\PYZus{}gradient\PYZus{}descent}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w\PYZus{}init}\PY{p}{,} \PY{n}{eta}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}2}\PY{p}{,} \PY{n}{max\PYZus{}iter}\PY{o}{=}\PY{l+m+mf}{1e4}\PY{p}{,}
                                         \PY{n}{min\PYZus{}weight\PYZus{}dist}\PY{o}{=}\PY{l+m+mf}{1e\PYZhy{}8}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{42}\PY{p}{,} \PY{n}{verbose}\PY{o}{=}\PY{n+nb+bp}{False}\PY{p}{)}\PY{p}{:}
             \PY{n}{weight\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{inf}
             \PY{n}{w} \PY{o}{=} \PY{n}{w\PYZus{}init}
             \PY{n}{errors} \PY{o}{=} \PY{p}{[}\PY{p}{]}
             \PY{n}{iter\PYZus{}num} \PY{o}{=} \PY{l+m+mi}{0}
             \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
             \PY{k}{while} \PY{n}{weight\PYZus{}dist} \PY{o}{\PYZgt{}} \PY{n}{min\PYZus{}weight\PYZus{}dist} \PY{o+ow}{and} \PY{n}{iter\PYZus{}num} \PY{o}{\PYZlt{}} \PY{n}{max\PYZus{}iter}\PY{p}{:}
                 \PY{n}{random\PYZus{}ind} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{n}{X}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}
                 \PY{n}{neww} \PY{o}{=} \PY{n}{stochastic\PYZus{}gradient\PYZus{}step}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{w}\PY{p}{,} \PY{n}{random\PYZus{}ind}\PY{p}{,} \PY{n}{eta}\PY{p}{)}
                 \PY{n}{errors}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{mserror}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{linear\PYZus{}prediction}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{)}\PY{p}{)}
                 \PY{n}{weight\PYZus{}dist} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linalg}\PY{o}{.}\PY{n}{norm}\PY{p}{(}\PY{n}{w} \PY{o}{\PYZhy{}} \PY{n}{neww}\PY{p}{)}
                 \PY{n}{w} \PY{o}{=} \PY{n}{neww}
                 \PY{n}{iter\PYZus{}num}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
             \PY{k}{return} \PY{n}{w}\PY{p}{,} \PY{n}{errors}
\end{Verbatim}


    \textbf{Запустите \(10^5\) итераций стохастического градиентного спуска.
Укажите вектор начальных весов \emph{w\_init}, состоящий из нулей.
Оставьте параметры \emph{eta} и \emph{seed} равными их значениям по
умолчанию (\emph{eta}=0.01, \emph{seed}=42 - это важно для проверки
ответов).}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}20}]:} \PY{o}{\PYZpc{}\PYZpc{}}\PY{k}{time}
         stoch\PYZus{}grad\PYZus{}desc\PYZus{}weights, stoch\PYZus{}errors\PYZus{}by\PYZus{}iter = stochastic\PYZus{}gradient\PYZus{}descent(X, y,
                                                                                     np.array([[0],[0],[0],[0]]), max\PYZus{}iter = 1e5)
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
CPU times: user 6.12 s, sys: 0 ns, total: 6.12 s
Wall time: 6.12 s

    \end{Verbatim}

    \textbf{Посмотрим, чему равна ошибка на первых 50 итерациях
стохастического градиентного спуска. Видим, что ошибка не обязательно
уменьшается на каждой итерации.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}21}]:} \PY{o}{\PYZpc{}}\PY{k}{pylab} inline
         \PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,} \PY{n}{stoch\PYZus{}errors\PYZus{}by\PYZus{}iter}\PY{p}{[}\PY{p}{:}\PY{l+m+mi}{50}\PY{p}{]}\PY{p}{)}
         \PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Populating the interactive namespace from numpy and matplotlib

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}21}]:} Text(0,0.5,u'MSE')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_45_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Теперь посмотрим на зависимость ошибки от номера итерации для
\(10^5\) итераций стохастического градиентного спуска. Видим, что
алгоритм сходится.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}22}]:} \PY{o}{\PYZpc{}}\PY{k}{pylab} inline
         \PY{n}{plot}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{stoch\PYZus{}errors\PYZus{}by\PYZus{}iter}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{stoch\PYZus{}errors\PYZus{}by\PYZus{}iter}\PY{p}{)}
         \PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Iteration number}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
         \PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{MSE}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
Populating the interactive namespace from numpy and matplotlib

    \end{Verbatim}

\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}22}]:} Text(0,0.5,u'MSE')
\end{Verbatim}
            
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \textbf{Посмотрим на вектор весов, к которому сошелся метод.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}23}]:} \PY{k}{print} \PY{n}{stoch\PYZus{}grad\PYZus{}desc\PYZus{}weights}
         \PY{k}{print} \PY{n}{norm\PYZus{}eq\PYZus{}weights}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
[[ 1.40190566e+01]
 [ 3.91069256e+00]
 [ 2.78209808e+00]
 [-8.10462217e-03]]
[[14.0225    ]
 [ 3.91925365]
 [ 2.79206274]
 [-0.02253861]]

    \end{Verbatim}

    \textbf{Посмотрим на среднеквадратичную ошибку на последней итерации.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}24}]:} \PY{n}{stoch\PYZus{}errors\PYZus{}by\PYZus{}iter}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\end{Verbatim}


\begin{Verbatim}[commandchars=\\\{\}]
{\color{outcolor}Out[{\color{outcolor}24}]:} 2.784412588352759
\end{Verbatim}
            
    \textbf{Какова среднеквадратичная ошибка прогноза значений Sales в виде
линейной модели с весами, найденными с помощью градиентного спуска?
Запишите ответ в файл '4.txt'.}

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}25}]:} \PY{n}{answer4} \PY{o}{=} \PY{n}{mserror}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{linear\PYZus{}prediction}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{stoch\PYZus{}grad\PYZus{}desc\PYZus{}weights}\PY{p}{)}\PY{p}{)}
         \PY{k}{print}\PY{p}{(}\PY{n}{answer4}\PY{p}{)}
         \PY{n}{write\PYZus{}answer\PYZus{}to\PYZus{}file}\PY{p}{(}\PY{n}{answer4}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{4.txt}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
2.784412588406704

    \end{Verbatim}

    \textbf{Ответами к заданию будут текстовые файлы, полученные в ходе
этого решения. Обратите внимание, что отправленные файлы не должны
содержать пустую строку в конце. Данный нюанс является ограничением
платформы Coursera. Мы работаем над исправлением этого ограничения.}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
